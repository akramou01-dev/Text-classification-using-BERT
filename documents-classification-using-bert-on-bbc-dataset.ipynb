{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"in this notebook we are going to classify some documents into 5 categs (sport, tech, entertainement , politics and Busness )\nfor this NLP task which is docs classification we are going to use BERT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch \nfrom transformers import BertTokenizer\nimport numpy as np\nfrom transformers import BertModel\nfrom tqdm import tqdm \n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:28.250064Z","iopub.execute_input":"2021-12-05T21:33:28.250803Z","iopub.status.idle":"2021-12-05T21:33:34.807534Z","shell.execute_reply.started":"2021-12-05T21:33:28.250693Z","shell.execute_reply":"2021-12-05T21:33:34.806824Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Intro\nin this notebook we are going to classify some documents into 5 categs (sport, tech, entertainement , politics and Busness )\nfor this NLP task which is docs classification we are going to use BERT","metadata":{"execution":{"iopub.status.busy":"2021-12-05T19:51:47.057184Z","iopub.execute_input":"2021-12-05T19:51:47.057924Z","iopub.status.idle":"2021-12-05T19:51:47.070542Z","shell.execute_reply.started":"2021-12-05T19:51:47.057888Z","shell.execute_reply":"2021-12-05T19:51:47.069433Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport torch \nfrom transformers import BertTokenizer\nimport numpy as np\nfrom transformers import BertModel\nfrom tqdm import tqdm ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:34.809248Z","iopub.execute_input":"2021-12-05T21:33:34.809483Z","iopub.status.idle":"2021-12-05T21:33:34.815670Z","shell.execute_reply.started":"2021-12-05T21:33:34.809451Z","shell.execute_reply":"2021-12-05T21:33:34.815042Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Loading  data","metadata":{}},{"cell_type":"code","source":"source_url = \"/kaggle/input/bbc-dataset/bbc-text.csv\"\ndf = pd.read_csv(source_url)\ndf.groupby([\"category\"]).size()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:34.816670Z","iopub.execute_input":"2021-12-05T21:33:34.817024Z","iopub.status.idle":"2021-12-05T21:33:34.958437Z","shell.execute_reply.started":"2021-12-05T21:33:34.816992Z","shell.execute_reply":"2021-12-05T21:33:34.957791Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"category\nbusiness         510\nentertainment    386\npolitics         417\nsport            511\ntech             401\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Preprocessing ","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-05T21:33:34.960689Z","iopub.execute_input":"2021-12-05T21:33:34.960969Z","iopub.status.idle":"2021-12-05T21:33:40.888452Z","shell.execute_reply.started":"2021-12-05T21:33:34.960921Z","shell.execute_reply":"2021-12-05T21:33:40.887824Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31244707f4324f4fa014687f5057a742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d01634f90c846259a229eb185416b11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fb9f61b9efa4c43b062315ce2fa6e86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"580a822d1519467fa0a0c4def4ec14e3"}},"metadata":{}}]},{"cell_type":"markdown","source":"we are going to tesst the bert tokenizer on an exemple to have a good idea on how it works","metadata":{}},{"cell_type":"code","source":"sample = 'Hey my name is BERT'\n# truncation : if it is True then we allow bert to truncated every sequence it's length is higher then max_length\n# return_tensors : the type of tensors that will be returned (as we are using pytorch then we set \"pt\")\nbert_input  = tokenizer(sample,padding=\"max_length\",max_length=15,truncation=True,return_tensors=\"pt\")\n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:40.889607Z","iopub.execute_input":"2021-12-05T21:33:40.890006Z","iopub.status.idle":"2021-12-05T21:33:40.902540Z","shell.execute_reply.started":"2021-12-05T21:33:40.889969Z","shell.execute_reply":"2021-12-05T21:33:40.901878Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(bert_input[\"input_ids\"])\n# input_ids are the id representation of each token \n# we can decode these inputs to get the original sequence \nprint(tokenizer.decode(bert_input[\"input_ids\"][0] ))\n# the code 102 is for the [SEP] token and the 0 is for [PAD] token ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:40.903882Z","iopub.execute_input":"2021-12-05T21:33:40.904356Z","iopub.status.idle":"2021-12-05T21:33:40.924101Z","shell.execute_reply.started":"2021-12-05T21:33:40.904320Z","shell.execute_reply":"2021-12-05T21:33:40.923312Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[ 101, 4403, 1139, 1271, 1110,  139, 9637, 1942,  102,    0,    0,    0,\n            0,    0,    0]])\n[CLS] Hey my name is BERT [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(bert_input[\"token_type_ids\"])\n# the token_type_ids identified to which sequence a token belongs, when having just one sequence so it's always 0 ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:40.925391Z","iopub.execute_input":"2021-12-05T21:33:40.925637Z","iopub.status.idle":"2021-12-05T21:33:40.931962Z","shell.execute_reply.started":"2021-12-05T21:33:40.925603Z","shell.execute_reply":"2021-12-05T21:33:40.931278Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(bert_input[\"attention_mask\"])\n# the attention_mask identified whether the token is a real word or just a token padding\n# it's 1 for the real words, the CLS and the SEP tokens, and for the pad token is 0\n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:40.933339Z","iopub.execute_input":"2021-12-05T21:33:40.933841Z","iopub.status.idle":"2021-12-05T21:33:40.939342Z","shell.execute_reply.started":"2021-12-05T21:33:40.933807Z","shell.execute_reply":"2021-12-05T21:33:40.938557Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"in this example, we are using the **bert-base-cased** pre-trained model and that's because our dataset is in english, if we are dealing with a dataset which is multilingual the we need to use **bert-base-multilingual-cased**","metadata":{}},{"cell_type":"markdown","source":"### Dataset Class \n\nNow that we know which are the ouputs of our bert tokenizer we are going to build a Dataset Class for our news Dataset ","metadata":{}},{"cell_type":"code","source":"tokenizer= BertTokenizer.from_pretrained(\"bert-base-cased\")\nlabels = {\n    'business':0,\n    'entertainment':1,\n    'sport':2,\n    'tech':3,\n    'politics':4\n}\nclass Dataset(torch.utils.data.Dataset): \n    def __init__(self,df): \n        #extract our labels from the df \n        self.labels = [labels[label] for label in df[\"category\"]]\n        #tokenize our texts to the format that BERT expects to get as input \n        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,return_tensors=\"pt\") for text in df[\"text\"]] \n    def classes(self):\n        return self.labels\n    \n    def __len__(self): \n        return len(self.labels)\n    \n    #fetch a batch of labels\n    def get_batch_labels(self,indx): \n        return np.array(self.labels[indx])\n    # fetch a batch of texts \n    def get_batch_texts(self,indx): \n        return self.texts[indx]\n\n    #get an item with the texts and the label\n    def __getitem__(self,indx): \n        batch_texts = self.get_batch_texts(indx)\n        batch_y = self.get_batch_labels(indx)\n        \n        \n        return batch_texts, batch_y\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:40.940669Z","iopub.execute_input":"2021-12-05T21:33:40.941192Z","iopub.status.idle":"2021-12-05T21:33:44.178425Z","shell.execute_reply.started":"2021-12-05T21:33:40.941156Z","shell.execute_reply":"2021-12-05T21:33:44.177668Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Now after creating the Dataset Class let's split our dataset into train,validation and test sets\n\n* training set contains : 80% \n* test and validation contains : 10% each","metadata":{}},{"cell_type":"code","source":"df_train, df_valid,df_test = np.split(df.sample(frac=1,random_state=42),[int(.8*len(df)), int(.9*len(df))])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:44.180862Z","iopub.execute_input":"2021-12-05T21:33:44.181444Z","iopub.status.idle":"2021-12-05T21:33:44.189269Z","shell.execute_reply.started":"2021-12-05T21:33:44.181404Z","shell.execute_reply":"2021-12-05T21:33:44.188364Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Building the model\nNow after preparing our data to the Learning process, let's create our model using the pre-trained BERT base model which contains 12 layers of Transformers encoder.","metadata":{}},{"cell_type":"code","source":"class BertClassifier(torch.nn.Module): \n    def __init__(self,dropout=0.5): \n        super(BertClassifier,self).__init__()\n        \n        self.bert=BertModel.from_pretrained(\"bert-base-cased\")\n        self.dropout = torch.nn.Dropout(dropout)\n        # bert output a vector of size 768\n        self.lin = torch.nn.Linear(768,5)\n        self.relu = torch.nn.ReLU()\n    def forward(self,input_id,mask): \n        # as output, the bert model give us first the embedding vector of all the tokens of the sequence \n        # second we get the embedding vector of the CLS token.\n        # fot a classification task it's enough to use this embedding for our classifier\n        _,pooled_output = self.bert(input_ids= input_id,attention_mask = mask,return_dict = False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output  = self.lin(dropout_output)\n        final_layer = self.relu(linear_output)\n        \n        return final_layer\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:44.190569Z","iopub.execute_input":"2021-12-05T21:33:44.190974Z","iopub.status.idle":"2021-12-05T21:33:44.200518Z","shell.execute_reply.started":"2021-12-05T21:33:44.190920Z","shell.execute_reply":"2021-12-05T21:33:44.199753Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training  ","metadata":{}},{"cell_type":"code","source":"# we are creating a standard pytorch training loop \n\ndef train(model, train_data, val_data, learning_rate, epochs=5):\n    #creating a custom Dataset objects using the training and validation data\n    train, val = Dataset(train_data), Dataset(val_data)\n    #creating dataloaders\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n\n    if use_cuda:\n            model = model.cuda()\n            criterion = criterion.cuda()\n\n    for epoch_num in range(epochs):\n\n            total_acc_train = 0\n            total_loss_train = 0\n\n            for train_input, train_label in tqdm(train_dataloader):\n                #print(f\"the train input : {train_input}\")\n                #print(f\"train label : {train_label}\")\n\n                train_label = train_label.to(device)\n                mask = train_input['attention_mask'].to(device)\n                input_id = train_input['input_ids'].squeeze(1).to(device)\n    #             print(input_id.shape)\n\n                # get the predictions \n                output = model(input_id, mask)\n\n                #the output is a vector of 5 values (categs)\n    #             print(output)\n    #             print(\"the output shape is\" ,  output.shape)\n    #             print(train_label)\n                \n                batch_loss = criterion(output, train_label)\n                total_loss_train += batch_loss.item()\n                \n                acc = (output.argmax(dim=1) == train_label).sum().item()\n                total_acc_train += acc\n                # updating the Gradient Descent and Backpropagation operation\n                model.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n            # now we evaluate on the validation data\n            total_acc_val = 0\n            total_loss_val = 0\n\n            with torch.no_grad():\n\n                for val_input, val_label in val_dataloader:\n\n                    val_label = val_label.to(device)\n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                    output = model(input_id, mask)\n\n                    batch_loss = criterion(output, val_label)\n                    total_loss_val += batch_loss.item()\n                    \n                    acc = (output.argmax(dim=1) == val_label).sum().item()\n                    total_acc_val += acc\n            \n            print(\n                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:33:44.203313Z","iopub.execute_input":"2021-12-05T21:33:44.204012Z","iopub.status.idle":"2021-12-05T21:33:44.219644Z","shell.execute_reply.started":"2021-12-05T21:33:44.203973Z","shell.execute_reply":"2021-12-05T21:33:44.218816Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"          \nEPOCHS = 5\nmodel = BertClassifier()\nlearning_rate = 1e-6\ntrain(model, df_train, df_valid, learning_rate, EPOCHS)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-05T21:33:44.220839Z","iopub.execute_input":"2021-12-05T21:33:44.221381Z","iopub.status.idle":"2021-12-05T21:45:03.326715Z","shell.execute_reply.started":"2021-12-05T21:33:44.221343Z","shell.execute_reply":"2021-12-05T21:45:03.325263Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e557c2876648b3a25d597830436903"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n100%|██████████| 890/890 [02:02<00:00,  7.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 1 | Train Loss:  0.726                 | Train Accuracy:  0.401                 | Val Loss:  0.623                 | Val Accuracy:  0.568\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 890/890 [02:01<00:00,  7.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 2 | Train Loss:  0.496                 | Train Accuracy:  0.678                 | Val Loss:  0.397                 | Val Accuracy:  0.734\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 890/890 [02:01<00:00,  7.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 3 | Train Loss:  0.247                 | Train Accuracy:  0.898                 | Val Loss:  0.120                 | Val Accuracy:  0.991\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 890/890 [02:01<00:00,  7.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 4 | Train Loss:  0.102                 | Train Accuracy:  0.978                 | Val Loss:  0.065                 | Val Accuracy:  0.991\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 890/890 [02:01<00:00,  7.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epochs: 5 | Train Loss:  0.056                 | Train Accuracy:  0.988                 | Val Loss:  0.058                 | Val Accuracy:  0.977\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## EValuate the model on test data\nNow that we trained the model on the training set, we are going to use the test data to evaluate the performance of the model on unseen data ","metadata":{}},{"cell_type":"code","source":"def evaluate(model,test_df):\n    test = Dataset(test_df)\n    test_dl = torch.utils.data.DataLoader(test,batch_size=2)\n    \n    cuda_available = torch.cuda.is_available()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n    if cuda_available:\n        model = model.cuda()\n    \n    total_acc = 0\n    for test_input , test_label in tqdm(test_dl):\n        test_label = test_label.to(device)\n        mask = test_input[\"attention_mask\"].to(device)\n        input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n        output = model(input_id,mask)\n        \n        acc = (output.argmax(dim=1) == test_label).sum().item()\n        total_acc +=acc \n        \n    print(f\"Test Accuracy : {total_acc / len(test_df): .3f}\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:45:03.329501Z","iopub.execute_input":"2021-12-05T21:45:03.329759Z","iopub.status.idle":"2021-12-05T21:45:03.337037Z","shell.execute_reply.started":"2021-12-05T21:45:03.329724Z","shell.execute_reply":"2021-12-05T21:45:03.336236Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"evaluate(model,test_df=df_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:45:03.338267Z","iopub.execute_input":"2021-12-05T21:45:03.338716Z","iopub.status.idle":"2021-12-05T21:45:11.025509Z","shell.execute_reply.started":"2021-12-05T21:45:03.338677Z","shell.execute_reply":"2021-12-05T21:45:11.024751Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 112/112 [00:04<00:00, 24.37it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy :  0.996\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"After running the code above, we got the accuracy of 0.991 from the test data.","metadata":{}},{"cell_type":"markdown","source":"## Custom Tests","metadata":{}},{"cell_type":"code","source":"device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninverse_labels = {v:k for k,v in labels.items()}","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:45:11.028441Z","iopub.execute_input":"2021-12-05T21:45:11.028651Z","iopub.status.idle":"2021-12-05T21:45:11.033135Z","shell.execute_reply.started":"2021-12-05T21:45:11.028624Z","shell.execute_reply":"2021-12-05T21:45:11.032119Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def predict(device,model,sentence):\n    sentence = sentence\n    sentence_input = tokenizer(sentence, padding='max_length', max_length=512, truncation=True,return_tensors=\"pt\").to(device)\n    input_id = sentence_input[\"input_ids\"]\n    mask = sentence_input[\"attention_mask\"]\n    output = model(input_id,mask)\n    predicted_class_label = output.argmax(dim=1)\n    predicted_class = inverse_labels[predicted_class_label.item()]\n    print(f\"The predicted class is : {predicted_class}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:45:11.034686Z","iopub.execute_input":"2021-12-05T21:45:11.034969Z","iopub.status.idle":"2021-12-05T21:45:11.043936Z","shell.execute_reply.started":"2021-12-05T21:45:11.034921Z","shell.execute_reply":"2021-12-05T21:45:11.043164Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"predict(device,model,\"a period of political and economic stability\")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:45:11.077405Z","iopub.execute_input":"2021-12-05T21:45:11.077699Z","iopub.status.idle":"2021-12-05T21:45:11.105519Z","shell.execute_reply.started":"2021-12-05T21:45:11.077664Z","shell.execute_reply":"2021-12-05T21:45:11.104796Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"The predicted class is : entertainment\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict(device,model,\"Manchester is the great football team in all the history\")","metadata":{"execution":{"iopub.status.busy":"2021-12-05T21:45:11.137019Z","iopub.execute_input":"2021-12-05T21:45:11.137249Z","iopub.status.idle":"2021-12-05T21:45:11.165677Z","shell.execute_reply.started":"2021-12-05T21:45:11.137217Z","shell.execute_reply":"2021-12-05T21:45:11.165013Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"The predicted class is : sport\n","output_type":"stream"}]},{"cell_type":"code","source":"predict(device,model,\"An individual or group can initiate, or obstruct, public policy in many political arenas\")","metadata":{},"execution_count":null,"outputs":[]}]}